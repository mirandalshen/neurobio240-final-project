{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "000459f5-1532-43ab-97d9-a1ba1babfe1b",
   "metadata": {},
   "source": [
    "For combined data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46089ca-9dd6-4334-a541-a33c9cbc76dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/shared/home/mis6559/neurobio240/BLIP')\n",
    "\n",
    "sys.argv = [\n",
    "    'decode_captions.py',\n",
    "    '--subject', 'subj01',\n",
    "    '--gpu', '0',\n",
    "    '--roi_str', 'early_ventral_midventral_midlateral_lateral_parietal'\n",
    "]\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from BLIP.models.blip import blip_decoder\n",
    "from transformers import logging\n",
    "import argparse\n",
    "\n",
    "logging.set_verbosity_error()  # suppress transformer warnings\n",
    "\n",
    "def generate_from_imageembeds(\n",
    "    model, device, image_embeds, sample=False, num_beams=3, max_length=30,\n",
    "    min_length=10, top_p=0.9, repetition_penalty=1.0\n",
    "):\n",
    "    # No repetition!\n",
    "    image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(device)\n",
    "\n",
    "    model_kwargs = {\n",
    "        \"encoder_hidden_states\": image_embeds,\n",
    "        \"encoder_attention_mask\": image_atts,\n",
    "    }\n",
    "\n",
    "    prompt = [model.prompt]\n",
    "    input_ids = model.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    input_ids[:, 0] = model.tokenizer.bos_token_id\n",
    "    input_ids = input_ids[:, :-1]  # Remove final token to avoid duplication\n",
    "\n",
    "    generate_args = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"max_length\": max_length,\n",
    "        \"min_length\": min_length,\n",
    "        \"eos_token_id\": model.tokenizer.sep_token_id,\n",
    "        \"pad_token_id\": model.tokenizer.pad_token_id,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "        **model_kwargs\n",
    "    }\n",
    "\n",
    "    if sample:\n",
    "        outputs = model.text_decoder.generate(\n",
    "            do_sample=True,\n",
    "            top_p=top_p,\n",
    "            num_return_sequences=1,\n",
    "            **generate_args\n",
    "        )\n",
    "    else:\n",
    "        outputs = model.text_decoder.generate(\n",
    "            num_beams=num_beams,\n",
    "            **generate_args\n",
    "        )\n",
    "\n",
    "    return [\n",
    "        model.tokenizer.decode(output, skip_special_tokens=True)[len(model.prompt):]\n",
    "        for output in outputs\n",
    "    ]\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--subject\", type=str, required=True)\n",
    "    parser.add_argument(\"--roi_str\", type=str, default=\"early_ventral_midlateral_midventral_lateral_parietal\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    subject = args.subject\n",
    "    roi_str = args.roi_str\n",
    "    gpu = args.gpu\n",
    "\n",
    "    torch.cuda.set_device(gpu)\n",
    "    device = torch.device(f\"cuda:{gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Setup paths\n",
    "    image_size = 240\n",
    "    decoded_path = f\"/shared/home/mis6559/neurobio240/decoded/{subject}/{subject}_{roi_str}_scores_blip_combined.npy\"\n",
    "    savedir = f\"/shared/home/mis6559/neurobio240/decoded/{subject}/captions\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "    # Load model\n",
    "    print(\"Loading BLIP decoder...\")\n",
    "    model_url = \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth\"\n",
    "    model_decoder = blip_decoder(\n",
    "        pretrained=model_url,\n",
    "        image_size=image_size,\n",
    "        vit=\"base\",\n",
    "        med_config=\"/shared/home/mis6559/neurobio240/BLIP/configs/med_config.json\"\n",
    "    )\n",
    "    model_decoder.eval()\n",
    "    model_decoder = model_decoder.to(device)\n",
    "\n",
    "    # Load predicted BLIP features\n",
    "    print(f\"Loading predicted features from: {decoded_path}\")\n",
    "    scores = np.load(decoded_path)\n",
    "    print(\"Shape:\", scores.shape)\n",
    "\n",
    "    # Use the fixed number of tokens (225) expected by BLIP\n",
    "    expected_tokens = 225\n",
    "    expected_length = expected_tokens * 768\n",
    "    print(f\"Using expected BLIP input shape: {expected_tokens} tokens ({expected_length} features)\")\n",
    "\n",
    "    # Generate captions\n",
    "    captions_brain = []\n",
    "    for imidx in tqdm(range(scores.shape[0])):\n",
    "        flat = scores[imidx, :]\n",
    "\n",
    "        if flat.shape[0] < expected_length:\n",
    "            raise ValueError(f\"[{imidx}] Too few features: {flat.shape[0]} < {expected_length}\")\n",
    "        elif flat.shape[0] > expected_length:\n",
    "            print(f\"[{imidx}] Trimming from {flat.shape[0] // 768} â†’ {expected_tokens} tokens\")\n",
    "            flat = flat[:expected_length]\n",
    "\n",
    "        scores_test = torch.Tensor(flat.reshape(expected_tokens, 768)).unsqueeze(0).to(device)\n",
    "\n",
    "        captions = generate_from_imageembeds(model_decoder, device, scores_test)\n",
    "        captions_brain.append(captions[0])\n",
    "\n",
    "    # Save output\n",
    "    out_csv = os.path.join(savedir, \"captions_brain.csv\")\n",
    "    pd.DataFrame(captions_brain).to_csv(out_csv, sep='\\t', header=False, index=False)\n",
    "    print(f\"Saved {len(captions_brain)} captions to: {out_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df11f28-f274-4de3-a5fb-fe54de52a028",
   "metadata": {},
   "source": [
    "For solo data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7123df-83b8-441a-b4d1-b2583b8a970d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/shared/home/mis6559/neurobio240/BLIP')\n",
    "\n",
    "sys.argv = [\n",
    "    'decode_captions.py',\n",
    "    '--subject', 'subj01',\n",
    "    '--gpu', '0',\n",
    "    '--roi_str', 'early'\n",
    "]\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from BLIP.models.blip import blip_decoder\n",
    "from transformers import logging\n",
    "import argparse\n",
    "\n",
    "logging.set_verbosity_error()  # suppress transformer warnings\n",
    "\n",
    "def generate_from_imageembeds(model, device, image_embeds, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0):\n",
    "    print(\"image_embeds shape BEFORE:\", image_embeds.shape)\n",
    "\n",
    "    prompt = [model.prompt]\n",
    "    input_ids = model.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    input_ids[:, 0] = model.tokenizer.bos_token_id\n",
    "    input_ids = input_ids[:, :-1]\n",
    "\n",
    "    if sample:\n",
    "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(device)\n",
    "        model_kwargs = {\n",
    "            \"encoder_hidden_states\": image_embeds,\n",
    "            \"encoder_attention_mask\": image_atts,\n",
    "        }\n",
    "        outputs = model.text_decoder.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            do_sample=True,\n",
    "            top_p=top_p,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=model.tokenizer.sep_token_id,\n",
    "            pad_token_id=model.tokenizer.pad_token_id,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            **model_kwargs)\n",
    "    else:\n",
    "        expanded_input_ids = input_ids.repeat_interleave(num_beams, dim=0)\n",
    "        expanded_image_embeds = image_embeds.repeat_interleave(num_beams, dim=0)\n",
    "        expanded_image_atts = torch.ones(expanded_image_embeds.size()[:-1], dtype=torch.long).to(device)\n",
    "\n",
    "        model_kwargs = {\n",
    "            \"encoder_hidden_states\": expanded_image_embeds,\n",
    "            \"encoder_attention_mask\": expanded_image_atts,\n",
    "        }\n",
    "\n",
    "        outputs = model.text_decoder.generate(\n",
    "            input_ids=expanded_input_ids,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            num_beams=num_beams,\n",
    "            eos_token_id=model.tokenizer.sep_token_id,\n",
    "            pad_token_id=model.tokenizer.pad_token_id,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            **model_kwargs)\n",
    "\n",
    "    captions = []\n",
    "    for output in outputs:\n",
    "        caption = model.tokenizer.decode(output, skip_special_tokens=True)\n",
    "        captions.append(caption[len(model.prompt):])\n",
    "    return captions\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--subject\", type=str, required=True)\n",
    "    parser.add_argument(\"--roi_str\", type=str, default=\"early_ventral_midlateral_midventral_lateral_parietal\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    subject = args.subject\n",
    "    roi_str = args.roi_str\n",
    "    gpu = args.gpu\n",
    "\n",
    "    torch.cuda.set_device(gpu)\n",
    "    device = torch.device(f\"cuda:{gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Setup paths\n",
    "    image_size = 240\n",
    "    decoded_path = f\"/shared/home/mis6559/neurobio240/decoded/{subject}/{subject}_early_scores_blip.npy\"\n",
    "    savedir = f\"/shared/home/mis6559/neurobio240/decoded/{subject}/captions\"\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "    # Load model\n",
    "    print(\"Loading BLIP decoder...\")\n",
    "    model_url = \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth\"\n",
    "    model_decoder = blip_decoder(\n",
    "        pretrained=model_url,\n",
    "        image_size=image_size,\n",
    "        vit=\"base\",\n",
    "        med_config=\"/shared/home/mis6559/neurobio240/BLIP/configs/med_config.json\"\n",
    "    )\n",
    "    model_decoder.eval()\n",
    "    model_decoder = model_decoder.to(device)\n",
    "\n",
    "    # Load BLIP predictions\n",
    "    print(f\"Loading predicted features from: {decoded_path}\")\n",
    "    scores = np.load(decoded_path)\n",
    "    print(f\"score shape: {scores.shape}\")  # should be (228, N), where N = num_tokens * 768\n",
    "    print(f\"Element count per image: {scores.shape[1]}\")\n",
    "    print(f\"Num tokens: {scores.shape[1] / 768}\")\n",
    "\n",
    "    \n",
    "    # Generate captions\n",
    "    captions_brain = []\n",
    "    for imidx in tqdm(range(scores.shape[0])):\n",
    "    \n",
    "        flat = scores[imidx, :]\n",
    "        assert flat.shape[0] == 226 * 768, f\"[{imidx}] Wrong vector length: {flat.shape[0]}\"\n",
    "        \n",
    "        reshaped = flat.reshape(226, 768)  # full 226 tokens\n",
    "        scores_test = torch.Tensor(reshaped).unsqueeze(0).to(device)\n",
    "        \n",
    "        print(f\"[{imidx}] Passing tensor with shape: {scores_test.shape}\")  # should be (1, 226, 768)\n",
    "    \n",
    "        caption = generate_from_imageembeds(model_decoder, device, scores_test, num_beams=3, max_length=20, min_length=5, repetition_penalty=1.5)\n",
    "        captions_brain.append(caption)\n",
    "    \n",
    "    # Save output\n",
    "    out_csv = os.path.join(savedir, \"captions_brain.csv\")\n",
    "    pd.DataFrame(captions_brain).to_csv(out_csv, sep='\\t', header=False, index=False)\n",
    "    print(f\"Saved decoded captions to: {out_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31b5c4-6023-48e2-9814-653ba41d2d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
