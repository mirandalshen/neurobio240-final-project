{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42347378-ebcc-4d93-b7c3-9f1464d2dbcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install transformers timm fairscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d0d76-b86b-4e74-922d-2d63c7b60dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/shared/home/mis6559/neurobio240/BLIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a857ec-ddbb-45e3-b846-2918e6b4116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BLIP.models.blip import blip_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aca45f-7f04-40c7-9e2e-e0e65d26eff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from models.blip import blip_decoder\n",
    "from nsd_access import NSDAccess\n",
    "\n",
    "# Parameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_size = 240\n",
    "n_images = 73000\n",
    "\n",
    "# Load BLIP model\n",
    "print(\"Loading BLIP model...\")\n",
    "model_url = \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth\"\n",
    "model = blip_decoder(\n",
    "    pretrained=model_url,\n",
    "    image_size=image_size,\n",
    "    vit=\"base\",\n",
    "    med_config=\"/shared/home/mis6559/neurobio240/BLIP/configs/med_config.json\"\n",
    ")\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "# Load NSD access\n",
    "print(\"Loading NSD image dataset...\")\n",
    "nsd_path = \"/shared/home/mis6559/neurobio240/nsd/\"\n",
    "nsda = NSDAccess(nsd_path)\n",
    "\n",
    "# Output directory\n",
    "outdir = \"/shared/home/mis6559/neurobio240/nsdfeat/blip/\"\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "# Process and save\n",
    "print(\"Extracting BLIP visual features...\")\n",
    "for idx in tqdm(range(n_images)):\n",
    "    out_path = os.path.join(outdir, f\"{idx:06}.npy\")\n",
    "    if os.path.exists(out_path):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        img_arr = nsda.read_images(idx)\n",
    "        image = Image.fromarray(img_arr).convert(\"RGB\")\n",
    "        image = image.resize((image_size, image_size), resample=Image.LANCZOS)\n",
    "        img_tensor = transforms.ToTensor()(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = model.visual_encoder(img_tensor)\n",
    "            features = features.squeeze().cpu().numpy()\n",
    "\n",
    "        np.save(out_path, features)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping image {idx}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da34d53-c0a1-46cd-adaf-519d0d0ae518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
