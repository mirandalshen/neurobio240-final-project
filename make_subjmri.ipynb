{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9465ec1-533c-4ed8-ad55-02d943f1c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nsd_access import NSDAccess\n",
    "import scipy.io\n",
    "import s3fs\n",
    "import nibabel as nb\n",
    "import io\n",
    "import sys\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_behavior_from_s3(subject, session_index):\n",
    "    s3 = s3fs.S3FileSystem(anon=True)\n",
    "    behavior_file_path = f\"s3://natural-scenes-dataset/nsddata/ppdata/{subject}/behav/responses.tsv\"\n",
    "    with s3.open(behavior_file_path, 'rb') as f:\n",
    "        behavior = pd.read_csv(f, delimiter='\\t')\n",
    "    session_behavior = behavior[behavior['SESSION'] == session_index]\n",
    "    return session_behavior\n",
    "\n",
    "def main():\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        subject = 'subj01'\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\"--subject\", type=str, default=\"subj01\", help=\"subject name\")\n",
    "        opt = parser.parse_args()\n",
    "        subject = opt.subject\n",
    "\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    atlasname = 'streams'\n",
    "    nsd_path = 's3://natural-scenes-dataset/'\n",
    "    nsda = NSDAccess(nsd_path)\n",
    "    nsd_expdesign = scipy.io.loadmat('nsd_expdesign.mat')\n",
    "    sharedix = nsd_expdesign['sharedix'] - 1\n",
    "\n",
    "    behs = pd.DataFrame()\n",
    "    for i in range(1, 38):\n",
    "        beh = read_behavior_from_s3(subject=subject, session_index=i)\n",
    "        behs = pd.concat((behs, beh))\n",
    "\n",
    "    stims_unique = behs['73KID'].unique() - 1\n",
    "    stims_all = behs['73KID'] - 1\n",
    "\n",
    "    savedir = f'./mrifeat/{subject}/'\n",
    "    os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(f'{savedir}/{subject}_stims.npy'):\n",
    "        np.save(f'{savedir}/{subject}_stims.npy', stims_all)\n",
    "        np.save(f'{savedir}/{subject}_stims_ave.npy', stims_unique)\n",
    "\n",
    "    session_save_dir = os.path.join(savedir, 'betas_sessions')\n",
    "    os.makedirs(session_save_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(1, 38):\n",
    "        print(f\"Processing session {i}\")\n",
    "        session_file = os.path.join(savedir, 'betas_sessions', f'betas_session{i:02d}.npy')\n",
    "    \n",
    "        needs_download = True\n",
    "        if os.path.exists(session_file):\n",
    "            try:\n",
    "                existing = np.load(session_file, mmap_mode='r')\n",
    "                if np.all(existing == 0) or np.isnan(existing).all():\n",
    "                    print(f\"Session file {session_file} is empty or invalid. Re-downloading.\")\n",
    "                else:\n",
    "                    print(f\"Already downloaded: {session_file}\")\n",
    "                    needs_download = False\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading existing {session_file}, re-downloading. Error: {e}\")\n",
    "    \n",
    "        if needs_download:\n",
    "            s3_path = f's3://natural-scenes-dataset/nsddata_betas/ppdata/{subject}/func1pt8mm/betas_fithrf_GLMdenoise_RR/betas_session{i:02d}.nii.gz'\n",
    "            try:\n",
    "                with fs.open(s3_path, 'rb') as f:\n",
    "                    gz = gzip.GzipFile(fileobj=f)\n",
    "                    raw_data = gz.read()\n",
    "                    img = nb.Nifti1Image.from_bytes(raw_data)\n",
    "                    beta_data = img.get_fdata(dtype=np.float32)\n",
    "                    beta_data = np.transpose(beta_data, (3, 1, 2, 0))  # (trials, x, y, z)\n",
    "\n",
    "                np.save(session_file, beta_data)\n",
    "                print(f\"Saved session {i} to {session_file}\")\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading session {i} from {s3_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "    print(\"Loading and concatenating session betas...\")\n",
    "    sample = np.load(os.path.join(session_save_dir, 'betas_session01.npy'))\n",
    "    print(\"Sample shape:\", sample.shape)\n",
    "\n",
    "    n_trials, x, y, z = sample.shape\n",
    "    total_trials = n_trials * 37\n",
    "    stims_all = stims_all[:total_trials]\n",
    "    behs = behs.iloc[:total_trials]\n",
    "\n",
    "    betas_all_path = f'{savedir}/{subject}_betas_all_memmap.npy'\n",
    "    if os.path.exists(betas_all_path):\n",
    "        print(f\"Memmap file already exists at {betas_all_path}, skipping concatenation.\")\n",
    "        betas_all = np.memmap(betas_all_path, dtype=np.float32, mode='r', shape=(total_trials, x, y, z))\n",
    "    else:\n",
    "        betas_all = np.memmap(betas_all_path, dtype=np.float32, mode='w+', shape=(total_trials, x, y, z))\n",
    "        offset = 0\n",
    "        for i in range(1, 38):\n",
    "            session_file = os.path.join(session_save_dir, f'betas_session{i:02d}.npy')\n",
    "            data = np.load(session_file)\n",
    "            n = data.shape[0]\n",
    "            betas_all[offset:offset + n] = data\n",
    "            offset += n\n",
    "        betas_all.flush()\n",
    "        print(f\"Memmap saved to {betas_all_path}\")\n",
    "\n",
    "    atlas_path = f's3://natural-scenes-dataset/nsddata/ppdata/{subject}/func1pt8mm/roi/{atlasname}.nii.gz'\n",
    "    with fs.open(atlas_path, 'rb') as f:\n",
    "        with gzip.GzipFile(fileobj=f) as gz:\n",
    "            raw_data = gz.read()\n",
    "            atlas_img = nb.Nifti1Image.from_bytes(raw_data)\n",
    "            atlas_data = atlas_img.get_fdata()\n",
    "\n",
    "    # Transpose to match beta volume shape\n",
    "    atlas_data = atlas_data.transpose(1, 2, 0)\n",
    "    \n",
    "    # Load ROI label mapping\n",
    "    mapping_path = f's3://natural-scenes-dataset/nsddata/freesurfer/fsaverage/label/{atlasname}.mgz.ctab'\n",
    "    with fs.open(mapping_path, 'rb') as f:\n",
    "        atlas_mapping_df = pd.read_csv(f, delimiter=' ', header=None, index_col=0)\n",
    "        atlas_mapping = atlas_mapping_df[1].to_dict()\n",
    "\n",
    "\n",
    "    print(\"Atlas mapping keys:\", atlas_mapping.keys())\n",
    "    print(\"Atlas mapping values:\", atlas_mapping.values())\n",
    "\n",
    "    atlas = (atlas_data, atlas_mapping)\n",
    "    atlas_flat = atlas[0].flatten()\n",
    "    print(\"Unique values in atlas_flat:\", np.unique(atlas_flat))\n",
    "    print(\"Type of atlas_flat[0]:\", type(atlas_flat[0]))\n",
    "\n",
    "\n",
    "    print(\"Atlas ROI voxel counts:\")\n",
    "    for label_id, roi_name in atlas[1].items():\n",
    "        print(f\"{label_id} ({roi_name}): {(atlas_flat == label_id).sum()}\")\n",
    "\n",
    "    betas_all = betas_all[:, :, :, :atlas_data.shape[2]]\n",
    "    print(\"betas_all shape:\", betas_all.shape)\n",
    "    print(\"atlas shape:\", atlas[0].shape)\n",
    "    print(\"betas_all shape:\", betas_all.shape)\n",
    "    assert betas_all.shape[1:] == atlas[0].shape, \"Beta and atlas spatial dimensions do not match!\"\n",
    "\n",
    "\n",
    "    # Show a visual check\n",
    "    z = 40\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(np.mean(betas_all[0, :, :, :], axis=2), cmap='gray')\n",
    "    plt.title('Mean Beta Volume (collapsed Z)')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(atlas[0][:, :, z], cmap='tab10')\n",
    "    plt.title(f'Atlas Slice z={z}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Unique atlas voxel values:\", np.unique(atlas[0]))\n",
    "\n",
    "    # Flatten beta volume\n",
    "    n_trials, x, y, z = betas_all.shape\n",
    "    betas_all_flat = betas_all.reshape(n_trials, -1)\n",
    "    print(\"betas_all_flat dtype:\", betas_all_flat.dtype)\n",
    "    print(\"betas_all_flat shape:\", betas_all_flat.shape)\n",
    "    print(\"betas_all_flat is memmap:\", isinstance(betas_all_flat, np.memmap))\n",
    "\n",
    "    print(\"Trying to read test chunk from betas_all_flat...\")\n",
    "    test_chunk = betas_all_flat[0:100, 0:100]\n",
    "    print(\"Test chunk shape:\", test_chunk.shape)\n",
    "\n",
    "    import psutil\n",
    "    \n",
    "    print(\"\\nStarting ROI processing...\")\n",
    "    \n",
    "    stim_to_idx = {stim: i for i, stim in enumerate(stims_unique)}  # faster lookup\n",
    "    \n",
    "    roi_indices_cache = {}\n",
    "    \n",
    "    for label_id, roi_name in atlas[1].items():\n",
    "        if label_id == 0:\n",
    "            print(f\"Skipping ROI {roi_name} (ID: 0)\")\n",
    "            continue\n",
    "    \n",
    "        print(f\"\\nProcessing ROI {roi_name} (ID: {label_id})\", flush=True)\n",
    "        \n",
    "        roi_mask = atlas_flat == label_id\n",
    "        voxel_count = roi_mask.sum()\n",
    "    \n",
    "        if voxel_count == 0:\n",
    "            print(f\"Skipping {roi_name}: no voxels found.\")\n",
    "            continue\n",
    "    \n",
    "        print(f\"  ROI voxel count: {voxel_count}\")\n",
    "        print(f\"  Available memory: {psutil.virtual_memory().available / 1e9:.2f} GB\")\n",
    "    \n",
    "        try:\n",
    "            # Use index list instead of boolean mask for performance\n",
    "            roi_indices = roi_indices_cache.get(label_id)\n",
    "            if roi_indices is None:\n",
    "                roi_indices = np.where(roi_mask)[0]\n",
    "                roi_indices_cache[label_id] = roi_indices\n",
    "    \n",
    "            # Load in chunks to avoid RAM overload\n",
    "            chunk_size = 5000\n",
    "            betas_roi_parts = []\n",
    "    \n",
    "            for start in range(0, len(stims_all), chunk_size):\n",
    "                end = min(start + chunk_size, len(stims_all))\n",
    "                part = betas_all_flat[start:end, roi_indices]\n",
    "                betas_roi_parts.append(part)\n",
    "    \n",
    "            betas_roi = np.vstack(betas_roi_parts).astype(np.float32)\n",
    "            print(f\"  betas_roi shape: {betas_roi.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading betas for {roi_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            # Averaging per unique stimulus\n",
    "            print(\"  Averaging betas by stimulus...\")\n",
    "            df = pd.DataFrame({'stim': stims_all})\n",
    "            betas_df = pd.DataFrame(betas_roi)\n",
    "            df = pd.concat([df, betas_df], axis=1)\n",
    "    \n",
    "            betas_roi_ave_df = df.groupby('stim').mean()\n",
    "            betas_roi_ave = betas_roi_ave_df.loc[stims_unique].values.astype(np.float32)\n",
    "            print(f\"  betas_roi_ave shape: {betas_roi_ave.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in averaging for {roi_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            # Train/test split\n",
    "            betas_tr = np.stack([betas_roi[idx] for idx, stim in enumerate(stims_all) if stim not in sharedix])\n",
    "            betas_te = np.stack([betas_roi[idx] for idx, stim in enumerate(stims_all) if stim in sharedix])\n",
    "            betas_ave_tr = np.stack([betas_roi_ave[idx] for idx, stim in enumerate(stims_unique) if stim not in sharedix])\n",
    "            betas_ave_te = np.stack([betas_roi_ave[idx] for idx, stim in enumerate(stims_unique) if stim in sharedix])\n",
    "            print(f\"  Shapes - tr: {betas_tr.shape}, te: {betas_te.shape}, ave_tr: {betas_ave_tr.shape}, ave_te: {betas_ave_te.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in train/test split for {roi_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            np.save(f'{savedir}/{subject}_{roi_name}_betas_tr.npy', betas_tr)\n",
    "            np.save(f'{savedir}/{subject}_{roi_name}_betas_te.npy', betas_te)\n",
    "            np.save(f'{savedir}/{subject}_{roi_name}_betas_ave_tr.npy', betas_ave_tr)\n",
    "            np.save(f'{savedir}/{subject}_{roi_name}_betas_ave_te.npy', betas_ave_te)\n",
    "            print(f\"  Saved all files for ROI {roi_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving files for {roi_name}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
